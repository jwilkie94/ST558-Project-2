---
title: "ST558-Project-2"
author: "Jenna Wilkie"
date: "10/22/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(randomForest)
library(gbm)
```


# Intoduction

The data set collected from different articles was published by Mashable (www.mashable.com) with the last two years. We mainly subset six different features: lifestyle, entainment, business, socmed, tech and world to predict the probility of online news popularity.  
In this project, we use four different methods - two linear models, a random forest and a boosted tree models - to model the response (shares of articles) from different articles on Mashable website and predict the popularity  of online news. 


# Reading in Data Set 

To begin, we read in the data set, then created a subset of the data to look at only the news article type that we are interested in.  


```{r data_set, message=FALSE}
online_news_popularity<-read_csv("OnlineNewsPopularity.csv")

#filter for data to select the chanel of interest
data<-online_news_popularity%>%filter(data_channel_is_bus == 1)
```


# Summarizations

Next we will look at some summary statistics of the data.

## Summary Statistics

To start, we want to take a look at some important summary statistics of the data.  


One interesting statistic to look at is the average number of shares since that is the variable that we will be predicting.  Below are the mean and standard deviation of that column.  The mean can tell us the expected value of the number of shares of each article, and the standard deviation can tell us how far the observed values vary from the expected value, on average.  A large standard deviation tells us that the data has a lot of variation.  

```{r token}
shares_avg<-mean(data$shares)
shares_sd<-sd(data$shares)

shares_avg
shares_sd
```

Another thing we are interested in is which day of the week has the most shares for this type of article and how many shares occur in total for each day of the week. The averages will give us the expected number of shares for each day, and the summation will give us a total count of the number of shares that have occured overall.  


```{r weekday}
mon_avg<-data%>%filter(weekday_is_monday == 1)
tue_avg<-data%>%filter(weekday_is_tuesday==1)
wed_avg<-data%>%filter(weekday_is_wednesday==1)
thu_avg<-data%>%filter(weekday_is_thursday==1)
fri_avg<-data%>%filter(weekday_is_friday==1)
sat_avg<-data%>%filter(weekday_is_saturday==1)
sun_avg<-data%>%filter(weekday_is_sunday==1)

mon_sum<-data%>%filter(weekday_is_monday == 1)
tue_sum<-data%>%filter(weekday_is_tuesday==1)
wed_sum<-data%>%filter(weekday_is_wednesday==1)
thu_sum<-data%>%filter(weekday_is_thursday==1)
fri_sum<-data%>%filter(weekday_is_friday==1)
sat_sum<-data%>%filter(weekday_is_saturday==1)
sun_sum<-data%>%filter(weekday_is_sunday==1)

avgs<-data.frame("Avg"=c(mean(mon_avg$shares),mean(tue_avg$shares),mean(wed_avg$shares),mean(thu_avg$shares),mean(fri_avg$shares),mean(sat_avg$shares),mean(sun_avg$shares)), "Day"=c("Mon", "Tues", "Wed", "Thur", "Fri", "Sat", "Sun"))

sums<-data.frame("Value"=c(sum(mon_sum$shares),sum(tue_sum$shares),sum(wed_sum$shares),sum(thu_sum$shares),sum(fri_sum$shares),sum(sat_sum$shares),sum(sun_sum$shares)), "Day"=c("Mon", "Tues", "Wed", "Thur", "Fri", "Sat", "Sun"))

avgs
sums
```





## Graphs

We want to see if there is any relation between the number of shares and the day of the week.  A good way to view that data may be to look at a graphical representation.  Below is a bar plot with the average counts for each day of the week.  A solid red bar indicates the total average number of shares without accounting for date..  

```{r weekday_plot}
# change the shape of the data for easy graphing.  
ggplot(data=avgs, aes(x=Day, y=Avg))+geom_bar(stat="Identity", fill="light blue")+geom_abline(aes(intercept=shares_avg,slope=0), color="red")+labs(x="Day of the Week", y="Number of Shares", title="Article Day of Week")+scale_x_discrete(labels=c("Monday", "Tuesday", "Wednesday","Thursday", "Friday", "Saturday","Sunday"))
```

Moving on from the days of the week, another interesting observation regarding the number of shares could involve the global_sentiment_polarity.  It is possible that more or less polarizing articles could show a difference in share rate. Below is a scatter plot of global_sentiment_polarity vs. shares. The correlation between the two variables in each plot is included.  When evaluating a correlation, a value that is closer to 1 or negative 1 indicates a stronger correlation.  A positive correlation means that as one variable increases so does the other, and a negative correlation means that as one variable increases, the other decreases.  For example, a correlation of -.77 would be a moderately strong negative correlation.  

```{r global_plot}
correlation<-cor(data$global_sentiment_polarity, data$shares)
ggplot(data, aes(x=global_sentiment_polarity, y=shares))+geom_point(color="pink")+labs(x="Global Sentiment Polarity", y="Shares" ,title="Global Polarity vs. Shares")+geom_text(x=0.5, y=6e+05, size=4 ,label=paste0("Correlation=",round(correlation,2)))
```

Next we can look at how keywords can affect the number of shares for an article.  The scatter plot below shows the kw_avg_avg plotted against shares.  The correlations are included on this plot and can be interpreted in the same way as stated above.  

```{r}
correlation<-cor(data$shares, data$kw_avg_avg)
ggplot(data, aes(x=shares, y=kw_avg_avg))+geom_point(color="#F8766D")+labs(x="Shares", y="Key Word Average" ,title="Key Word Average vs. Shares")+geom_text(x=6e+05, y=40000, size=3 ,label=paste0("Correlation=",round(correlation,2)))
```

# Modeling

Based on the trends above, and common knowledge, we will fit a two linear models, a random forest model, and a boosted tree model for comparison.  

The first step is to split the data into a training and test set using the **caret** package.

```{r split}
# set seed for reproduceability
set.seed(89)

train_index<-createDataPartition(data$shares, p=0.7, list=FALSE)
training<-data[train_index,]
test<-data[-train_index,]
```  

Now each of the models will be fitted against the training data set and analyzed through 10 fold cross validation.  

## Linear Regression Models

A linear regression model allows us to predict the response of a variable of interest, in this case the **shares** variable, based on a selected set of predictors.  A simple linear regression model can be written as y=Beta_0+Beta_1*x, where Beta_0 is the intercept, Beta_1 is the slope, x is the predictor variable, and y is the response.  Multiple linear regression models can be used in instances where multiple possible predictors are present.  In those cases there will be a slope for each predictor to account for the effect that predictor can have on the response.  

```{r lrm}
set.seed(89)

linear_fit_1 <- train(shares~self_reference_avg_sharess+weekday_is_tuesday+weekday_is_wednesday+weekday_is_thursday+global_rate_positive_words:global_rate_negative_words+num_imgs+num_videos, data=data, method = "lm", trControl=trainControl(method="cv", number=10),preProcess = c("center", "scale"))

```  

```{r multiple linear model}
set.seed(89)

linear_fit_2 <- train(shares~self_reference_avg_sharess+weekday_is_tuesday+weekday_is_wednesday+weekday_is_thursday+global_rate_positive_words:global_rate_negative_words+num_imgs+num_videos, data=data, method = "glm", trControl=trainControl(method="cv", number=10),preProcess = c("center", "scale"))
```  


## Ensemble Models

### Random Forest Model

A random forest model creates multiple trees from bootstrap analysis then averages the results.  Random forest models randomly select a subset of predictors for averaging, so the results won't be affected by one or two very strong predictors.  

```{r rf}
set.seed(89)
random_forest_fit <- train(shares~self_reference_avg_sharess+weekday_is_tuesday+weekday_is_wednesday+weekday_is_thursday+global_rate_positive_words:global_rate_negative_words+num_imgs+num_videos, data=data, method = "rf", trControl=trainControl(method="cv", number=10),preProcess = c("center", "scale"), tuneGrid=data.frame(mtry=length(data)/3))

```  

### Boosted Tree Model 

A boosted tree model was created. Ensemble boosting tree approach can be applied to make trees grown sequentially, each subsequent tree grown on a modified version of original data (slow fitting because of shrinkage), and predictions updated as trees grown. Because the tree grown was shrink for slow fitting, so the appropriate interpretation of trees was lost. 

```{r boosted}
set.seed(89)
boosted_tree_fit <-
  train(shares ~ self_reference_avg_sharess+weekday_is_tuesday+weekday_is_wednesday+weekday_is_thursday+global_rate_positive_words:global_rate_negative_words+num_imgs+num_videos, data=training, method = "gbm", trControl=trainControl(method="cv", number=10),preProcess = c("center", "scale"), tuneGrid=expand.grid(n.trees=c(10,20,50,100,200), interaction.depth = c(1,2,3,4), shrinkage = 0.1, n.minobsinnode = 10))

## prediction of test data set. 
boost_predict <- predict(boosted_tree_fit, newdata = test)
confusionMatrix(boost_predict, test$shares)
plot(boosted_tree_fit)

```  

